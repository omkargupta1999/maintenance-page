[root@drlogging1 dashboards]# cat README.md
# Loki HA Grafana Dashboards - Comprehensive Guide

## ðŸ“Š **Dashboard Overview**

This directory contains comprehensive Grafana dashboards for monitoring the Loki High Availability deployment with detailed descriptions and explanations for each panel and graph.

## ðŸŽ¯ **Dashboard Categories**

### **1. Summary Dashboard**

#### **Summary** (`summary.json`)
- **Purpose**: Comprehensive overview of both nodes and all components in one unified view
- **Key Metrics**: Service status, ring members, memory chunks, error rate, log ingestion, query rate, ring status
- **Use Case**: Single dashboard for complete system monitoring and health check

##### **Panel Descriptions:**

**Service Status Panel**
- **What it shows**: Health status of all Loki services across both nodes
- **Values**: Green (1) = Healthy, Red (0) = Down
- **Monitors**: Distributors, ingesters, query frontend, queriers, compactor, Prometheus
- **Thresholds**: Green when value = 1, Red when value = 0

**Ring Members Panel**
- **What it shows**: Number of active members in the Loki ring
- **Expected**: 15 members per node
- **Components**: Each node has 7 components (1 compactor + 2 distributors + 2 ingesters + 2 queriers)
- **Thresholds**: Green = Optimal (12+), Yellow = Some missing (10-11), Red = Critical (<10)
- **Purpose**: Monitor cluster membership health

**Memory Chunks Panel**
- **What it shows**: Total number of chunks stored in memory by ingesters
- **Purpose**: Monitor memory usage and data processing load
- **Thresholds**: Green = Normal (<1000), Yellow = High (1000-5000), Red = Critical (>5000)
- **Action**: Higher values indicate more data being processed

**Error Rate Panel**
- **What it shows**: Rate of errors per second from distributors
- **Purpose**: Monitor system health and identify issues
- **Thresholds**: Green = No errors (0), Yellow = Low error rate (0.1), Red = High error rate (1+)
- **Action**: Investigate when errors occur

**Log Ingestion Rate Graph**
- **What it shows**: Rate of log data ingestion in bytes per second
- **Purpose**: Monitor data ingestion performance
- **Trends**: Higher values = more log data being processed
- **Action**: Watch for ingestion bottlenecks

**Query Rate Graph**
- **What it shows**: Rate of queries per second processed by query frontends
- **Purpose**: Monitor query performance and load
- **Trends**: Higher values = more query activity
- **Action**: Monitor for query performance issues

**Ring Members Status Graph**
- **What it shows**: Number of active ring members over time
- **Purpose**: Monitor cluster stability
- **Trends**: Stable values = healthy cluster, Drops = node failures
- **Action**: Investigate drops in membership

### **2. System Overview Dashboards**

#### **System Overview** (`system-overview.json`)
- **Purpose**: High-level system health and performance overview
- **Key Metrics**: Service status, log ingestion rate, query rate, ring members, memory chunks, error rate
- **Use Case**: Quick system health check and performance monitoring

##### **Panel Descriptions:**

**Service Health Status**
- **What it shows**: Overall health of all Loki services
- **Values**: Green = All services healthy, Red = Any service down
- **Purpose**: Quick health check for entire system

**Log Ingestion Overview**
- **What it shows**: Total log ingestion rate across all distributors
- **Purpose**: Monitor data ingestion performance
- **Trends**: Consistent rates indicate healthy ingestion

**Query Performance Overview**
- **What it shows**: Total query rate across all query frontends
- **Purpose**: Monitor query processing performance
- **Trends**: Stable rates indicate good query performance

**Ring Health Overview**
- **What it shows**: Ring membership status across cluster
- **Purpose**: Monitor cluster stability
- **Trends**: Stable membership indicates healthy cluster

**Memory Usage Overview**
- **What it shows**: Total memory chunks across all ingesters
- **Purpose**: Monitor memory consumption
- **Trends**: Increasing trends may indicate memory pressure

**Error Rate Overview**
- **What it shows**: Total error rate across all components
- **Purpose**: Monitor system health
- **Trends**: Low error rates indicate healthy system

#### **Cluster Overview** (`cluster-overview.json`)
- **Purpose**: Comprehensive cluster-wide metrics and health
- **Key Metrics**: Log ingestion by node, query rate by node, ring status, memory chunks, compaction runs, error rates
- **Use Case**: Detailed cluster monitoring and performance analysis

##### **Panel Descriptions:**

**Node Comparison - Log Ingestion**
- **What it shows**: Log ingestion rate comparison between Node 1 and Node 2
- **Purpose**: Identify performance differences between nodes
- **Trends**: Similar rates indicate balanced load distribution

**Node Comparison - Query Rate**
- **What it shows**: Query rate comparison between Node 1 and Node 2
- **Purpose**: Monitor query load distribution
- **Trends**: Balanced rates indicate good load distribution

**Ring Status by Node**
- **What it shows**: Ring membership status for each node
- **Purpose**: Monitor individual node ring participation
- **Trends**: Consistent participation indicates healthy nodes

**Memory Usage by Node**
- **What it shows**: Memory chunk usage for each node
- **Purpose**: Monitor memory distribution across nodes
- **Trends**: Balanced usage indicates good load distribution

**Compaction Performance**
- **What it shows**: Compaction runs and performance by node
- **Purpose**: Monitor storage optimization
- **Trends**: Regular compaction indicates healthy storage

**Error Rate by Node**
- **What it shows**: Error rates for each node
- **Purpose**: Identify problematic nodes
- **Trends**: Low error rates indicate healthy nodes

### **3. Performance Analysis Dashboards**

#### **Performance Comparison** (`performance-comparison.json`)
- **Purpose**: Side-by-side comparison of Node 1 vs Node 2 performance
- **Key Metrics**: Log ingestion comparison, query rate comparison, ring members comparison, memory chunks comparison, error rate comparison, compaction runs comparison
- **Use Case**: Identifying performance differences between nodes

##### **Panel Descriptions:**

**Log Ingestion Comparison**
- **What it shows**: Direct comparison of log ingestion rates between nodes
- **Purpose**: Identify load balancing issues
- **Analysis**: Similar rates = good load balancing, Different rates = potential issues

**Query Rate Comparison**
- **What it shows**: Direct comparison of query rates between nodes
- **Purpose**: Monitor query load distribution
- **Analysis**: Balanced rates = good distribution, Imbalanced = potential issues

**Ring Members Comparison**
- **What it shows**: Ring membership comparison between nodes
- **Purpose**: Monitor cluster participation
- **Analysis**: Similar participation = healthy cluster

**Memory Usage Comparison**
- **What it shows**: Memory usage comparison between nodes
- **Purpose**: Monitor resource utilization
- **Analysis**: Balanced usage = good resource distribution

**Error Rate Comparison**
- **What it shows**: Error rate comparison between nodes
- **Purpose**: Identify problematic nodes
- **Analysis**: Low error rates = healthy nodes

**Compaction Comparison**
- **What it shows**: Compaction performance comparison between nodes
- **Purpose**: Monitor storage optimization
- **Analysis**: Regular compaction = healthy storage

#### **Query Analytics** (`query-analytics.json`)
- **Purpose**: Detailed query performance analysis
- **Key Metrics**: Query latency percentiles, query throughput, query error rate, active queries, query cache hits, query cache misses
- **Use Case**: Query optimization and performance troubleshooting

##### **Panel Descriptions:**

**Query Latency Percentiles**
- **What it shows**: 50th, 95th, and 99th percentile query latencies
- **Purpose**: Monitor query performance
- **Analysis**: Lower percentiles = better performance

**Query Throughput**
- **What it shows**: Number of queries processed per second
- **Purpose**: Monitor query processing capacity
- **Analysis**: Higher throughput = better capacity

**Query Error Rate**
- **What it shows**: Rate of failed queries
- **Purpose**: Monitor query reliability
- **Analysis**: Low error rates = reliable queries

**Active Queries**
- **What it shows**: Number of currently running queries
- **Purpose**: Monitor query concurrency
- **Analysis**: High concurrency = good performance

**Cache Hit Rate**
- **What it shows**: Percentage of queries served from cache
- **Purpose**: Monitor cache effectiveness
- **Analysis**: High hit rates = efficient caching

**Cache Miss Rate**
- **What it shows**: Percentage of queries requiring backend processing
- **Purpose**: Monitor cache efficiency
- **Analysis**: Low miss rates = efficient caching

### **4. Fault Tolerance Dashboard**

#### **Fault Tolerance** (`fault-tolerance.json`)
- **Purpose**: Monitor fault tolerance and recovery mechanisms
- **Key Metrics**: Ring members status, heartbeat failures, ring cleanup operations, ring join operations, ring leave operations, ring suspect operations
- **Use Case**: Ensuring system resilience and automatic recovery

##### **Panel Descriptions:**

**Ring Members Status**
- **What it shows**: Number of active ring members over time
- **Purpose**: Monitor cluster stability
- **Analysis**: Stable membership = healthy cluster

**Heartbeat Failures**
- **What it shows**: Number of heartbeat failures
- **Purpose**: Monitor node communication health
- **Analysis**: Low failures = good communication

**Ring Cleanup Operations**
- **What it shows**: Number of ring cleanup operations
- **Purpose**: Monitor automatic recovery
- **Analysis**: Regular cleanup = healthy recovery

**Ring Join Operations**
- **What it shows**: Number of nodes joining the ring
- **Purpose**: Monitor cluster expansion
- **Analysis**: Joins indicate healthy expansion

**Ring Leave Operations**
- **What it shows**: Number of nodes leaving the ring
- **Purpose**: Monitor cluster contraction
- **Analysis**: Leaves may indicate node failures

**Ring Suspect Operations**
- **What it shows**: Number of nodes marked as suspect
- **Purpose**: Monitor fault detection
- **Analysis**: Suspects indicate potential issues

### **5. Storage Dashboards**

#### **Storage Trends** (`storage-trends.json`)
- **Purpose**: Monitor storage performance and compaction
- **Key Metrics**: Compaction runs, compaction duration, compaction errors, compaction success, compaction bytes processed, compaction chunks processed
- **Use Case**: Storage optimization and performance tuning

##### **Panel Descriptions:**

**Compaction Runs**
- **What it shows**: Number of compaction operations over time
- **Purpose**: Monitor storage optimization activity
- **Analysis**: Regular runs = healthy storage

**Compaction Duration**
- **What it shows**: Time taken for compaction operations
- **Purpose**: Monitor compaction performance
- **Analysis**: Shorter durations = better performance

**Compaction Errors**
- **What it shows**: Number of failed compaction operations
- **Purpose**: Monitor storage health
- **Analysis**: Low errors = healthy storage

**Compaction Success Rate**
- **What it shows**: Percentage of successful compaction operations
- **Purpose**: Monitor storage reliability
- **Analysis**: High success rates = reliable storage

**Compaction Bytes Processed**
- **What it shows**: Amount of data processed during compaction
- **Purpose**: Monitor storage workload
- **Analysis**: Higher bytes = more data optimization

**Compaction Chunks Processed**
- **What it shows**: Number of chunks processed during compaction
- **Purpose**: Monitor storage efficiency
- **Analysis**: More chunks = better optimization

#### **Storage** (`storage.json`)
- **Purpose**: Monitor S3-compatible storage backend
- **Key Metrics**: MinIO disk usage, MinIO request rate, MinIO error rate, bucket size
- **Use Case**: Storage backend monitoring and capacity planning

##### **Panel Descriptions:**

**MinIO Disk Usage**
- **What it shows**: Disk space usage by MinIO
- **Purpose**: Monitor storage capacity
- **Analysis**: Monitor for capacity limits

**MinIO Request Rate**
- **What it shows**: Number of requests per second to MinIO
- **Purpose**: Monitor storage activity
- **Analysis**: Higher rates = more storage activity

**MinIO Error Rate**
- **What it shows**: Rate of errors from MinIO
- **Purpose**: Monitor storage health
- **Analysis**: Low errors = healthy storage

**Bucket Size**
- **What it shows**: Size of Loki data buckets
- **Purpose**: Monitor data storage
- **Analysis**: Growing sizes = more data storage

### **6. Component Dashboards**

#### **Distributor** (`distributor.json`)
- **Purpose**: Monitor distributor components
- **Key Metrics**: Bytes received rate, lines received rate, active ingester clients, ingester appends rate, replication factor
- **Use Case**: Distributor performance monitoring

##### **Panel Descriptions:**

**Bytes Received Rate**
- **What it shows**: Rate of bytes received by distributors
- **Purpose**: Monitor data ingestion performance
- **Analysis**: Higher rates = more data ingestion

**Lines Received Rate**
- **What it shows**: Rate of log lines received by distributors
- **Purpose**: Monitor log ingestion performance
- **Analysis**: Higher rates = more log processing

**Active Ingester Clients**
- **What it shows**: Number of active ingester connections
- **Purpose**: Monitor distributor-ingester connectivity
- **Analysis**: Stable connections = healthy communication

**Ingester Appends Rate**
- **What it shows**: Rate of appends to ingesters
- **Purpose**: Monitor data flow to ingesters
- **Analysis**: Higher rates = more data flow

**Replication Factor**
- **What it shows**: Current replication factor setting
- **Purpose**: Monitor data redundancy
- **Analysis**: Higher factors = more redundancy

#### **Ingester** (`ingester.json`)
- **Purpose**: Monitor ingester components
- **Key Metrics**: Memory chunks, memory streams, chunks created rate, streams created rate, flush queue length, WAL bytes in use
- **Use Case**: Ingester performance monitoring

##### **Panel Descriptions:**

**Memory Chunks**
- **What it shows**: Number of chunks stored in memory
- **Purpose**: Monitor memory usage
- **Analysis**: Higher values = more memory usage

**Memory Streams**
- **What it shows**: Number of streams stored in memory
- **Purpose**: Monitor stream processing
- **Analysis**: More streams = more active processing

**Chunks Created Rate**
- **What it shows**: Rate of chunk creation
- **Purpose**: Monitor data processing
- **Analysis**: Higher rates = more data processing

**Streams Created Rate**
- **What it shows**: Rate of stream creation
- **Purpose**: Monitor stream processing
- **Analysis**: Higher rates = more stream activity

**Flush Queue Length**
- **What it shows**: Number of chunks waiting to be flushed
- **Purpose**: Monitor flush performance
- **Analysis**: Longer queues = potential flush delays

**WAL Bytes in Use**
- **What it shows**: Amount of WAL (Write-Ahead Log) data
- **Purpose**: Monitor WAL usage
- **Analysis**: Higher usage = more WAL activity

#### **Query Frontend** (`query-frontend.json`)
- **Purpose**: Monitor query frontend components
- **Key Metrics**: Query enqueue rate, queue length, queue duration, connected clients, cache hit rate, cache miss rate
- **Use Case**: Query frontend performance monitoring

##### **Panel Descriptions:**

**Query Enqueue Rate**
- **What it shows**: Rate of queries being enqueued
- **Purpose**: Monitor query processing
- **Analysis**: Higher rates = more query activity

**Queue Length**
- **What it shows**: Number of queries waiting in queue
- **Purpose**: Monitor query backlog
- **Analysis**: Longer queues = potential delays

**Queue Duration**
- **What it shows**: Time queries spend in queue
- **Purpose**: Monitor query processing time
- **Analysis**: Shorter durations = better performance

**Connected Clients**
- **What it shows**: Number of connected query clients
- **Purpose**: Monitor client connectivity
- **Analysis**: More clients = higher usage

**Cache Hit Rate**
- **What it shows**: Percentage of queries served from cache
- **Purpose**: Monitor cache effectiveness
- **Analysis**: Higher rates = better caching

**Cache Miss Rate**
- **What it shows**: Percentage of queries requiring backend processing
- **Purpose**: Monitor cache efficiency
- **Analysis**: Lower rates = better caching

#### **Querier** (`querier.json`)
- **Purpose**: Monitor querier components
- **Key Metrics**: Query frontend clients, lines sent to ingester rate, request duration
- **Use Case**: Querier performance monitoring

##### **Panel Descriptions:**

**Query Frontend Clients**
- **What it shows**: Number of connections to query frontend
- **Purpose**: Monitor querier-frontend connectivity
- **Analysis**: Stable connections = healthy communication

**Lines Sent to Ingester Rate**
- **What it shows**: Rate of data sent to ingesters
- **Purpose**: Monitor data flow
- **Analysis**: Higher rates = more data flow

**Request Duration**
- **What it shows**: Time taken for query requests
- **Purpose**: Monitor query performance
- **Analysis**: Shorter durations = better performance

#### **Compactor** (`compactor.json`)
- **Purpose**: Monitor compactor components
- **Key Metrics**: Compactor running, compact operations rate, compact operation duration
- **Use Case**: Compactor performance monitoring

##### **Panel Descriptions:**

**Compactor Running**
- **What it shows**: Whether compactor is actively running
- **Purpose**: Monitor compactor status
- **Analysis**: Running = active optimization

**Compact Operations Rate**
- **What it shows**: Rate of compaction operations
- **Purpose**: Monitor compaction activity
- **Analysis**: Higher rates = more optimization

**Compact Operation Duration**
- **What it shows**: Time taken for compaction operations
- **Purpose**: Monitor compaction performance
- **Analysis**: Shorter durations = better performance

### **7. Node-Specific Dashboards**

#### **Node 1** (`node1.json`)
- **Purpose**: Monitor Node 1 specific metrics
- **Key Metrics**: Node 1 specific performance and health metrics, recent logs
- **Use Case**: Node 1 specific monitoring

##### **Panel Descriptions:**

**Node 1 - Bytes Received Rate**
- **What it shows**: Log ingestion rate for Node 1
- **Purpose**: Monitor Node 1 ingestion performance
- **Analysis**: Higher rates = better performance

**Node 1 - Lines Received Rate**
- **What it shows**: Log line processing rate for Node 1
- **Purpose**: Monitor Node 1 processing capacity
- **Analysis**: Higher rates = more capacity

**Node 1 - Memory Chunks**
- **What it shows**: Memory usage for Node 1
- **Purpose**: Monitor Node 1 memory consumption
- **Analysis**: Monitor for memory pressure

**Node 1 - Query Enqueue Rate**
- **What it shows**: Query processing rate for Node 1
- **Purpose**: Monitor Node 1 query performance
- **Analysis**: Higher rates = better query performance

**Node 1 - Recent Logs**
- **What it shows**: Recent log entries from Node 1
- **Purpose**: Monitor Node 1 log activity
- **Analysis**: Recent logs = active processing

#### **Node 2** (`node2.json`)
- **Purpose**: Monitor Node 2 specific metrics
- **Key Metrics**: Node 2 specific performance and health metrics, recent logs
- **Use Case**: Node 2 specific monitoring

##### **Panel Descriptions:**

**Node 2 - Bytes Received Rate**
- **What it shows**: Log ingestion rate for Node 2
- **Purpose**: Monitor Node 2 ingestion performance
- **Analysis**: Higher rates = better performance

**Node 2 - Lines Received Rate**
- **What it shows**: Log line processing rate for Node 2
- **Purpose**: Monitor Node 2 processing capacity
- **Analysis**: Higher rates = more capacity

**Node 2 - Memory Chunks**
- **What it shows**: Memory usage for Node 2
- **Purpose**: Monitor Node 2 memory consumption
- **Analysis**: Monitor for memory pressure

**Node 2 - Query Enqueue Rate**
- **What it shows**: Query processing rate for Node 2
- **Purpose**: Monitor Node 2 query performance
- **Analysis**: Higher rates = better query performance

**Node 2 - Recent Logs**
- **What it shows**: Recent log entries from Node 2
- **Purpose**: Monitor Node 2 log activity
- **Analysis**: Recent logs = active processing

## ðŸ”§ **Dashboard Configuration**

### **Data Sources**
- **Prometheus-Node1**: Node 1 Prometheus metrics (http://localhost:9090)
- **Prometheus-Node2**: Node 2 Prometheus metrics (http://localhost:9091)
- **Loki-Node1**: Node 1 Loki query endpoint (http://localhost:9205)
- **Loki-Node2**: Node 2 Loki query endpoint (http://localhost:9301)
- **AlertManager**: Alert management (http://localhost:9093)

### **Refresh Intervals**
- **All Dashboards**: 30s
- **Real-time Monitoring**: Continuous updates

### **Time Ranges**
- **Default**: Last 1 hour
- **Available**: 5m, 15m, 30m, 1h, 3h, 6h, 12h, 24h, 7d, 30d

## ðŸ“ˆ **Key Metrics Explained**

### **Health Metrics**
- **up**: Service availability (1 = up, 0 = down)
- **loki_ring_members**: Number of active ring members
- **loki_distributor_errors_total**: Total distributor errors
- **loki_query_frontend_queries_total**: Total queries processed

### **Performance Metrics**
- **loki_distributor_received_bytes_total**: Total bytes received
- **loki_ingester_memory_chunks**: Memory chunks in use
- **loki_query_frontend_query_duration_seconds**: Query processing time
- **loki_compactor_compaction_runs_total**: Total compaction runs

### **Storage Metrics**
- **minio_disk_usage_bytes**: MinIO disk usage
- **redis_memory_used_bytes**: Redis memory usage
- **loki_compactor_compaction_bytes_processed**: Compaction data processed

## ðŸš¨ **Alerting Integration**

All dashboards are integrated with Prometheus alerting rules:
- **Critical Alerts**: Service down, high error rates, ring unhealthy
- **Warning Alerts**: High latency, high ingestion rates, compaction errors
- **Infrastructure Alerts**: Disk usage, memory usage, network issues

## ðŸ“š **Usage Guidelines**

### **Primary Dashboard**
1. **Start with Summary** - Complete system overview in one dashboard
2. **Use for daily monitoring** - All key metrics in one place
3. **Quick health checks** - Service status and performance at a glance

### **Detailed Analysis**
1. **System Overview** - High-level system health
2. **Cluster Overview** - Detailed cluster monitoring
3. **Performance Comparison** - Node-to-node comparison

### **Component Monitoring**
1. **Distributor** - Log ingestion monitoring
2. **Ingester** - Memory and storage monitoring
3. **Query Frontend** - Query performance monitoring
4. **Querier** - Query processing monitoring
5. **Compactor** - Storage optimization monitoring

### **Node-Specific Monitoring**
1. **Node 1** - Node 1 specific metrics and logs
2. **Node 2** - Node 2 specific metrics and logs

### **Troubleshooting**
1. **Fault Tolerance** - System resilience and recovery
2. **Storage Trends** - Storage performance issues
3. **Query Analytics** - Query performance issues

## ðŸ”„ **Maintenance**

### **Dashboard Updates**
- Dashboards are automatically provisioned by Grafana
- Updates require restarting the observability stack
- Configuration changes are version controlled

### **Customization**
- All dashboards are editable in Grafana
- Custom panels can be added as needed
- Alert thresholds can be adjusted based on requirements

## ðŸ“‹ **Dashboard List**

### **Primary Dashboards**
- **Summary** - Complete system overview
- **System Overview** - High-level health
- **Cluster Overview** - Detailed cluster metrics

### **Analysis Dashboards**
- **Performance Comparison** - Node comparison
- **Query Analytics** - Query performance

### **Fault Tolerance Dashboard**
- **Fault Tolerance** - System resilience

### **Storage Dashboards**
- **Storage Trends** - Compaction monitoring
- **Storage** - S3 backend monitoring

### **Component Dashboards**
- **Distributor** - Log ingestion
- **Ingester** - Memory and storage
- **Query Frontend** - Query optimization
- **Querier** - Query processing
- **Compactor** - Storage optimization

### **Node Dashboards**
- **Node 1** - Node 1 specific
- **Node 2** - Node 2 specific
